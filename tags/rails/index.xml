<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Rails on {code that works}</title>
    <link>https://sadique.io/tags/rails/?utm_source=site&amp;utm_medium=feed</link>
    <description>Recent content in Rails on {code that works}</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 13 Oct 2013 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://sadique.io/tags/rails/?utm_source=site&amp;utm_medium=feed" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Implementing Rate Limiting in Rails - Part 2</title>
      <link>https://sadique.io/blog/2013/10/13/implementing-rate-limiting-in-rails-part-2/?utm_source=site&amp;utm_medium=feed</link>
      <pubDate>Sun, 13 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>https://sadique.io/blog/2013/10/13/implementing-rate-limiting-in-rails-part-2/?utm_source=site&amp;utm_medium=feed</guid>
      <description>&lt;p&gt;&lt;em&gt;The first part of this series can be found &lt;a href=&#34;https://sadique.io/blog/2013/10/12/implementing-rate-limiting-in-rails-part-1/&#34;&gt;here&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The first part of this series looked at how to implement basic rate limiting in a Rails application. However, as pointed out in the improvements section, the implementation was not complete - it did not provide clients enough information about the rate limiting that is in place and how long they should wait before making further requests once they hit the limit.&lt;/p&gt;
&lt;p&gt;In order to tell the client about the rate limit parameters, the mechanism needs to be able to set headers on the response. While a &lt;code&gt;before_filter&lt;/code&gt; is useful to limit the requests, it can not change the response from a valid request. One could use an &lt;code&gt;after_filter&lt;/code&gt; to achieve this, but a Rack middleware &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; is a more suitable solution given that middlewares can act up on a request as well as the response generated by the application for that request.&lt;/p&gt;
&lt;p&gt;We will need to comment out the &lt;code&gt;before_filter&lt;/code&gt; that was introduced in Part 1. Then we will define a blank middleware and wire it up. The convention is to define middlwares in &lt;code&gt;app/middleware&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;
# app/middleware/rate_limit.rb

class RateLimit
  def initialize(app)
    @app = app
  end

  def call(env)
    @app.env
  end
end

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This middleware is wired up as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;
# config/application.rb

class Application &amp;lt; Rails::Application
  ...
  config.middleware.use &amp;quot;RateLimit&amp;quot;
end
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;basic-rate-limiting&#34;&gt;Basic Rate Limiting&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s re-implement what we implemented in Part 1 using the middleware.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;  def call(env)
    client_ip = env[&amp;quot;REMOTE_ADDR&amp;quot;]
    key = &amp;quot;count:#{client_ip}&amp;quot;
    count = REDIS.get(key)
    unless count
      REDIS.set(key, 0)
      REDIS.expire(key, THROTTLE_TIME_WINDOW)
    end

    if count.to_i &amp;gt;= THROTTLE_MAX_REQUESTS
      [
       429,
       {},
       [message]
      ]
    else
      REDIS.incr(key)
      @app.call(env)
    end
  end

  private
  def message
    {
      :message =&amp;gt; &amp;quot;You have fired too many requests. Please wait for some time.&amp;quot;
    }.to_json
  end

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;rate-limit-status&#34;&gt;Rate limit status&lt;/h3&gt;
&lt;p&gt;There are various header conventions for providing a client it&amp;rsquo;s rate limit status. For this example, we will use the convention that GitHub &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; and Twitter &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; use. The following headers represent the rate limit status:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;X-RateLimit-Limit&lt;/code&gt; - The maximum number of requests that the client is permitted to make in the time window.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;X-RateLimit-Remaining&lt;/code&gt; - The number of requests remaining in the current rate limit window.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;X-RateLimit-Reset&lt;/code&gt; - The time at which the current rate limit window resets in UTC epoch seconds &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The middleware will set these headers for all requests with the following change:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;  def call(env)
    client_ip = env[&amp;quot;REMOTE_ADDR&amp;quot;]
    key = &amp;quot;count:#{client_ip}&amp;quot;
    count = REDIS.get(key)
    unless count
      REDIS.set(key, 0)
      REDIS.expire(key, THROTTLE_TIME_WINDOW)
    end

    if count.to_i &amp;gt;= THROTTLE_MAX_REQUESTS
      [
       429,
       rate_limit_headers(count, key),
       [message]
      ]
    else
      REDIS.incr(key)
      status, headers, body = @app.call(env)
      [
       status,
       headers.merge(rate_limit_headers(count.to_i + 1, key)),
       body
      ]
    end
  end

  private
  def message
    {
      :message =&amp;gt; &amp;quot;You have fired too many requests. Please wait for some time.&amp;quot;
    }.to_json
  end

  def rate_limit_headers(count, key)
    ttl = REDIS.ttl(key)
    time = Time.now.to_i
    time_till_reset = (time + ttl.to_i).to_s
    {
      &amp;quot;X-Rate-Limit-Limit&amp;quot; =&amp;gt;  &amp;quot;60&amp;quot;,
      &amp;quot;X-Rate-Limit-Remaining&amp;quot; =&amp;gt; (60 - count.to_i).to_s,
      &amp;quot;X-Rate-Limit-Reset&amp;quot; =&amp;gt; time_till_reset
    }
  end

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This computes the time remaining till the limit is reset and the number of requests remaining and sets the appropriate headers.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s test this.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash$ for i in {1..100}
do
curl -i http://localhost:3000/foo.json &amp;gt;&amp;gt; /tmp/headers.log
done

bash$ less /tmp/headers.log | grep X-Rate-Limit
X-Rate-Limit-Limit: 60
X-Rate-Limit-Remaining: 59
X-Rate-Limit-Reset: 1381717125
X-Rate-Limit-Limit: 60
X-Rate-Limit-Remaining: 58
X-Rate-Limit-Reset: 1381717125
...
X-Rate-Limit-Limit: 60
X-Rate-Limit-Remaining: 1
X-Rate-Limit-Reset: 1381717124
X-Rate-Limit-Limit: 60
X-Rate-Limit-Remaining: 0
X-Rate-Limit-Reset: 1381717124
X-Rate-Limit-Limit: 60
X-Rate-Limit-Remaining: 0
X-Rate-Limit-Reset: 1381717124
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code for this implementation is on my &lt;a href=&#34;https://github.com/sdqali/rails_throttle&#34;&gt;GitHub profile&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;RailsCast &lt;a href=&#34;http://railscasts.com/episodes/151-rack-middleware&#34;&gt;#151 - Rack Middleware&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;GitHub API V3 - &lt;a href=&#34;http://developer.github.com/v3/#rate-limiting&#34;&gt;Rate limiting&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;Twitter - &lt;a href=&#34;https://dev.twitter.com/docs/rate-limiting/1.1&#34;&gt;REST API Rate Limiting in v1.1&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;Wikipedia - Unix time - &lt;a href=&#34;https://en.wikipedia.org/wiki/Unix_time#Encoding_time_as_a_number&#34;&gt;Encoding time as a number&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Implementing Rate Limiting in Rails - Part 1</title>
      <link>https://sadique.io/blog/2013/10/12/implementing-rate-limiting-in-rails-part-1/?utm_source=site&amp;utm_medium=feed</link>
      <pubDate>Sat, 12 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>https://sadique.io/blog/2013/10/12/implementing-rate-limiting-in-rails-part-1/?utm_source=site&amp;utm_medium=feed</guid>
      <description>&lt;p&gt;&lt;em&gt;The second part of this series can be found &lt;a href=&#34;https://sadique.io/blog/2013/10/13/implementing-rate-limiting-in-rails-part-2/&#34;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Rate limiting or throttling is the practice of limiting how frequently legitimate users of a web service can access the service. Rate limiting is often put in place to prevent the hogging of resources by a sub set of the users of the system. Rate limiting works by responding with error messages when a client exceeds their allocated share of requests within a predefined time window. In addition to the error message, the response should also include information as to when rate limit will be reset so that the client can continue accessing the system after the reset.&lt;/p&gt;
&lt;p&gt;What we need is a way to record the number of requests each client is making and reset this number to zero after a predefined time period and decide for each request whether the client making the request has exceeded the limits.&lt;/p&gt;
&lt;h3 id=&#34;the-test-application&#34;&gt;The test application&lt;/h3&gt;
&lt;p&gt;We will create a simple Rails application with a single API endpoint at &lt;code&gt;foo.json&lt;/code&gt;. The code needed for this is as follows:&lt;/p&gt;
&lt;p&gt;Routes:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# config/routes.rb

RailsThrottle::Application.routes.draw do
  get &#39;foo.json&#39; =&amp;gt; &#39;foo#index&#39;
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The controller:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# app/controllers/foo_controller.rb

class FooController &amp;lt; ApplicationController
  def index
    render json: {foo: :bar}
  end
end
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;storing-the-rate-data&#34;&gt;Storing the rate data&lt;/h3&gt;
&lt;p&gt;We need a place to store each client&amp;rsquo;s IP address and the number of requests it made. We need to increment this count for each request and reset the count to zero after a time period. Considering these needs, &lt;a href=&#34;http://redis.io/&#34;&gt;Redis&lt;/a&gt; is a great fit for this data store. Redis stores key value pairs and allows expiry time to be specified for each entry. Redis also comes with an &lt;code&gt;INCR&lt;/code&gt; &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; command that ensures that increment operations are atomic. This will be useful to us if we were to run multiple instances of our app behind a load balancer.&lt;/p&gt;
&lt;p&gt;To setup the application to use Redis, we will need to install the &lt;code&gt;redis&lt;/code&gt; &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; gem. Once we have the gem, we will add a new &lt;code&gt;initializer&lt;/code&gt; named &lt;code&gt;throttle.rb&lt;/code&gt; which configures our Redis client.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# config/initializers/throttle.rb

require &amp;quot;redis&amp;quot;

redis_conf  = YAML.load(File.join(Rails.root, &amp;quot;config&amp;quot;, &amp;quot;redis.yml&amp;quot;))
REDIS = Redis.new(:host =&amp;gt; redis_conf[&amp;quot;host&amp;quot;], :port =&amp;gt; redis_conf[&amp;quot;port&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will load the Redis server&amp;rsquo;s host and port from the configuration file located at &lt;code&gt;config/redis.yml&lt;/code&gt;. This file will look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# config/redis.yml

host: localhost
port: 6379
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;using-a-before_filter-for-rate-limiting&#34;&gt;Using a &lt;code&gt;before_filter&lt;/code&gt; for rate limiting.&lt;/h3&gt;
&lt;p&gt;The first step is to log the number of requests each client is making. This can easily be achieved with a &lt;code&gt;before_filter&lt;/code&gt; &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. Let&amp;rsquo;s add the filter into the &lt;code&gt;ApplicationController&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# app/controllers/application_controller.rb

class ApplicationController &amp;lt; ActionController::Base
  ...

  before_filter :throttle

  def throttle
    client_ip = request.env[&amp;quot;REMOTE_ADDR&amp;quot;]
    key = &amp;quot;count:#{client_ip}&amp;quot;
    count = REDIS.get(key)

    unless count
      REDIS.set(key, 0)
    end
    REDIS.incr(key)
    true
  end

  ...
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since this &lt;code&gt;before_filter&lt;/code&gt; belongs to the &lt;code&gt;ApplicationController&lt;/code&gt;, it will be applied to all requests, unless a specific controller chooses to skip it. So before every request is processed, the filter grabs the client&amp;rsquo;s IP and checks whether there is a count in Redis for this IP. If there is no count key, it creates one. Finally it increments the count.&lt;/p&gt;
&lt;p&gt;At this point, the filter just records the requests made, but does not limit requests. Let&amp;rsquo;s go ahead and implement limiting. We need to specify the time window for rate limiting and how many requests should be allowed in that time window. We will allow a client a maximum of &lt;code&gt;60&lt;/code&gt; requests in &lt;code&gt;15&lt;/code&gt; minutes. The following constants need to be defined in &lt;code&gt;throttle.rb&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;THROTTLE_TIME_WINDOW = 15 * 60
THROTTLE_MAX_REQUESTS = 60
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The filter needs to be changed to respond with error messages when the rate limit is exceeded.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# app/controllers/application_controller.rb

class ApplicationController &amp;lt; ActionController::Base
  ...

  before_filter :throttle

  def throttle
    client_ip = request.env[&amp;quot;REMOTE_ADDR&amp;quot;]
    key = &amp;quot;count:#{client_ip}&amp;quot;
    count = REDIS.get(key)

    unless count
      REDIS.set(key, 0)
      REDIS.expire(key, THROTTLE_TIME_WINDOW)
      return true
    end

    if count.to_i &amp;gt;= THROTTLE_MAX_REQUESTS
      render :status =&amp;gt; 429, :json =&amp;gt; {:message =&amp;gt; &amp;quot;You have fired too many requests. Please wait for some time.&amp;quot;}
      return
    end
    REDIS.incr(key)
    true
  end

  ...
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When the limit is reached, subsequent requests will be responded with an error message and the HTTP status code &lt;code&gt;429&lt;/code&gt;. The 429 &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; status code indicates that the user has sent too many requests in a given amount of time.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s go ahead and test this.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash$ for i in {1..100}
do
curl -i http://localhost:3000/foo.json &amp;gt;&amp;gt; /dev/null
done

bash$ less log/development.log | grep &amp;quot;200 OK&amp;quot; | wc -l
      60

bash$ less log/development.log | grep &amp;quot;429 Too Many Requests&amp;quot; | wc -l
      40

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see after &lt;code&gt;60&lt;/code&gt; requests, all requests get the &lt;code&gt;429&lt;/code&gt; response.&lt;/p&gt;
&lt;h3 id=&#34;improvements&#34;&gt;Improvements&lt;/h3&gt;
&lt;p&gt;While what we have implemented limits the requests, it does not give the client enough information as to how long it has to wait before making requests again. It would also be helpful if the server tells the client on each request how many total requests it is allowed to make in a window and how many more requests it can perform before limiting kicks in. We will look at this in the next blog post.&lt;/p&gt;
&lt;p&gt;A sample application with this rate limiting in place is on &lt;a href=&#34;https://github.com/sdqali/rails_throttle/tree/filter&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://redis.io/commands/incr&#34;&gt;Redis documentation&lt;/a&gt; for INCR command.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://rubygems.org/gems/redis&#34;&gt;redis&lt;/a&gt; - A Ruby client that tries to match Redis&amp;rsquo; API one-to-one, while still providing an idiomatic interface. It features thread-safety, client-side sharding, pipelining, and an obsession for performance.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;Rails&amp;rsquo; &lt;a href=&#34;http://apidock.com/rails/AbstractController/Callbacks/ClassMethods/before_filter&#34;&gt;before filter&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;IETF: Additional HTTP Status Codes - &lt;a href=&#34;https://tools.ietf.org/html/rfc6585#section-4&#34;&gt;429 Too Many Requests&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Why your project should have a Getting Started guide.</title>
      <link>https://sadique.io/blog/2012/01/17/why-your-project-should-have-a-getting-started-guide./?utm_source=site&amp;utm_medium=feed</link>
      <pubDate>Tue, 17 Jan 2012 00:00:00 +0000</pubDate>
      
      <guid>https://sadique.io/blog/2012/01/17/why-your-project-should-have-a-getting-started-guide./?utm_source=site&amp;utm_medium=feed</guid>
      <description>&lt;p&gt;My new team at work is writing a bunch of Rails applications. This is one of those codebases that one would call &amp;ldquo;legacy&amp;rdquo; without much argument. Most of these apps have their own patched, vendorized Rails versions.&lt;/p&gt;
&lt;p&gt;Getting up and running was an absolute pain. This project existed before Bundler and the list of gem dependencies are not checked in. I got the output of running &lt;code&gt;gem list&lt;/code&gt; on a colleague&amp;rsquo;s box, wrote a Ruby script to generate a shell script that installs all the gems. When I tried running the tests in one of those apps, I got a nice error.&lt;/p&gt;
&lt;p&gt;It looked obvious that we were using patched Rails versions. Surprisingly, theses were not checked in to the &lt;code&gt;vendor/&lt;/code&gt; directory. This was proving to be a pain, and today I sat with a team member and wrote a developer guide to get started on the project. We actually had to pull patched Rails tar balls from a remote box and untar them to &lt;code&gt;vendor/&lt;/code&gt; directory. I was really surprised that these patches had not been checked in. We checked in all the patches. Apart from the Rails patches, there was a gem that had to be checked in.&lt;/p&gt;
&lt;p&gt;This is why I think every project needs a Getting Started guide:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;As a developer joining a new team, I want to look at the code as soon as I can. For me, this involves reading and running the tests.&lt;/li&gt;
&lt;li&gt;Due to various reasons, a project may have patches and dependencies. Being very specific to the project, there is no way a new developer on the team would figure out these hidden dependencies.&lt;/li&gt;
&lt;li&gt;There needs to be a single place where all the dependencies are specified.&lt;/li&gt;
&lt;li&gt;While spoon-feeding someone may not be the best way to get them started, a little bit of hand-holding will not do anyone any harm.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;One of the biggest problems I see with things like these guides is that as the application and it&amp;rsquo;s dependencies evolve, the guides are not updated to reflect these changes. The only possible time the guide will be updated is when a new developer joins the team, follows these instructions and finds the need to update the guide.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Debugging: C Sharp&#39;s HttpWebRequest, 100-Continue and nginx</title>
      <link>https://sadique.io/blog/2012/01/16/debugging-c-sharps-httpwebrequest-100-continue-and-nginx/?utm_source=site&amp;utm_medium=feed</link>
      <pubDate>Mon, 16 Jan 2012 00:00:00 +0000</pubDate>
      
      <guid>https://sadique.io/blog/2012/01/16/debugging-c-sharps-httpwebrequest-100-continue-and-nginx/?utm_source=site&amp;utm_medium=feed</guid>
      <description>&lt;p&gt;Recently I spent some time debugging an issue our team was facing around some C# code making a request on one of our servers. The request was throwing a &lt;code&gt;The server committed a protocol violation. Section=ResponseStatusLine&lt;/code&gt; error.&lt;/p&gt;
&lt;p&gt;Initial investigation suggested that this could happen if we are making HTTP/1.1 requests to a server configured for HTTP/1.0. Our Rails application runs on Mongrel fronted with nginx 0.6.5. We modified the C# code to use HTTP/1.0 and the error went away. The following line does the trick.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c#&#34;&gt;request.ProtocolVersion = HttpVersion.Version10;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But wait! This means that somewhere in the chain, a server is configured to use HTTP/1.0. It looked unlikely and further debugging revealed that it was indeed not the case. Further staring at the Rails logs showed that one of the headers that the app expects was not being set, when the request was done using HTTP/1.1 from the code.&lt;/p&gt;
&lt;p&gt;After some time, we figured &lt;a href=&#34;http://stackoverflow.com/questions/2482715/the-server-committed-a-protocol-violation-section-responsestatusline-error&#34;&gt;out&lt;/a&gt; that the .Net library throws the &lt;code&gt;server committed ...&lt;/code&gt; error if it is expecting the HTTP 100 (Continue) response in the wrong way. We set the code to not expect the HTTP 100 response from the server using&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c#&#34;&gt;request.ServicePoint.Expect100Continue = false;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and voila, it worked. The Rails app received all the headers it expected and things worked fine. The code looked like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c#&#34;&gt;var request = (HttpWebRequest) HttpWebRequest.Create(&amp;quot;https://example.com/foo&amp;quot;);
request.ServicePoint.Expect100Continue = false;
request.Credentials = new NetworkCredential(&amp;quot;user&amp;quot;, &amp;quot;password&amp;quot;);
request.Method = &amp;quot;PUT&amp;quot;;
request.ContentType = &amp;quot;application/x-www-form-urlencoded&amp;quot;;
byte[] objByteArray = Encoding.UTF8.GetBytes(&amp;quot;foo=bar&amp;quot;);
request.ContentLength = objByteArray.Length;
var dataStream = request.GetRequestStream();
dataStream.Write(objByteArray, 0, objByteArray.Length);
dataStream.Close();

var response = request.GetResponse();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So what is happening?&lt;/p&gt;
&lt;p&gt;The HTTP 100 status is supposed to work like this. When a client has to send some data, instead of sending it upfront, it can send some headers along with the &amp;ldquo;Expect:100-Continue&amp;rdquo; header. The server responds with a 100 if it is willing to accept the request or send a final status. The spec is &lt;a href=&#34;http://www.w3.org/Protocols/rfc2616/rfc2616-sec8.html#sec8.2.3&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We are using nginx as a proxy. The specification says that the proxy should forward the request if it knows that the next-hop server is HTTP/1.1 compliant. The proxy is supposed to ignore the &amp;ldquo;Expect:100-Continue&amp;rdquo; header, if the request came from a client using HTTP/1.0.&lt;/p&gt;
&lt;p&gt;In our case, the default behavior of the .Net HTTP client library is to set &amp;ldquo;Expect:100-Continue&amp;rdquo; header on every request for HTTP/1.1. So the client sends only some headers and waits for the 100 response from nginx. Nginx sees the request, knows that Mongrel supports HTTP/1.1 and just forwards the request. The app sends a 401 because it could not authenticate. The client is expecting a 100 and gets a 401. It thinks the server committed a protocol violation.&lt;/p&gt;
&lt;p&gt;When we ask the client to use HTTP/1.0, the .Net library does not use the Expect header, sends all the headers and nginx forwards the request to Mongrel. The authentication goes through.&lt;/p&gt;
&lt;p&gt;When we explicitly set the Expect 100 property of the library to false, it sends all the headers at once and the authentication goes through fine.&lt;/p&gt;
&lt;p&gt;Looks like there is a way to tell .Net not to expect 100 from the server through configuration, by putting this in &lt;app&gt;.exe.conf&lt;/app&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;system.net&amp;gt;
  &amp;lt;settings&amp;gt;
    &amp;lt;servicePointManager expect100Continue=&amp;quot;false&amp;quot;/&amp;gt;
  &amp;lt;/settings&amp;gt;
&amp;lt;/system.net&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://stackoverflow.com/questions/2482715/the-server-committed-a-protocol-violation-section-responsestatusline-error&#34;&gt;http://stackoverflow.com/questions/2482715/the-server-committed-a-protocol-violation-section-responsestatusline-error&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.w3.org/Protocols/rfc2616/rfc2616-sec8.html#sec8.2.3&#34;&gt;http://www.w3.org/Protocols/rfc2616/rfc2616-sec8.html#sec8.2.3&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
